## Week 4: Linear Transformations and Their Applications

This week focuses on understanding linear algebra concepts through **linear transformations**, leading up to **Eigenvalues and Eigenvectors** and their application in **Principal Component Analysis (PCA)**.

### Principal Component Analysis (PCA)

* **Dimensionality Reduction**: PCA is a technique to reduce the number of dimensions (features/columns) in a dataset while preserving as much useful information as possible.
* **Simplification**: It simplifies complex datasets, making them easier to use and visualize.
* **How it works**: Imagine data points in a 2D space that mostly lie close to a line. PCA finds this line and projects all data points onto it, transforming a 2D dataset into a 1D dataset with minimal information loss. 
* **Application**: Widely used in data science and machine learning.

### Lesson 1: Characterizing Linear Transformations

This lesson focuses on understanding the properties of linear transformations using determinants.

* **Singularity**:
    * A linear transformation can be **singular** or **non-singular**.
    * **Non-singular transformation**: Transforms a space without collapsing it (e.g., plane to plane).
    * **Singular transformation**: Collapses a space to a lower dimension (e.g., plane to a line).
    * This concept is closely related to the singularity of a matrix.
* **Determinant**:
    * **Geometric Interpretation**: The determinant quantifies how much a linear transformation stretches or shrinks space.
    * **Properties**:
        * Useful for analyzing inverse transformations.
        * Helpful when dealing with a cascade of multiple transformations.
        * Properties exist for determinants of matrix products and inverses.

### Lesson 2: Basis, Span, Eigenvectors, and Eigenvalues (Leading to PCA)

This lesson introduces fundamental concepts required to understand PCA.

* **Basis**:
    * A set of vectors that define a space.
    * Any point in a space can be reached by a linear combination of its basis vectors.
    * Example: Two non-collinear vectors form a basis for a 2D plane.
* **Span**:
    * Describes the space that can be generated by linear combinations of a group of vectors.
    * A single vector spans a line.
    * Two non-collinear vectors span a plane.
* **Eigenvectors and Eigenvalues**:
    * **Eigenvectors**: Special directions (vectors) in a space.
    * **Eigenvalues**: Constants associated with eigenvectors.
    * **Property**: Applying a linear transformation to a point along its eigenvector simply scales the point along that same vector; it doesn't change its direction. This means multiplying by a matrix is equivalent to multiplying by a constant (the eigenvalue) for points on the eigenvector.
    * **Significance**: Eigenvectors help characterize linear transformations and are crucial in many machine learning applications, including PCA.

## Singular and Non-Singular Linear Transformations

Linear transformations, like matrices, can be **singular** or **non-singular**. This property relates to the **image** of the transformation and its **rank**.

### Image of a Linear Transformation

* The **image** of a linear transformation is the set of all possible output points when the transformation is applied to every point in the input space.

### Non-Singular Transformations

* A linear transformation is **non-singular** if its image covers the entire output space.
* **Geometric Interpretation**: If a transformation maps a basis (like a square grid) to a new set of basis vectors that still span the entire plane (forming a parallelogram that covers all points), it is non-singular. Example: Following matrix transforms a grid into a stretched grid that still covers the entire plane.

$$
\begin{bmatrix}
3 & 1 \\
1 & 2
\end{bmatrix}
$$
        
        
* **Rank**: For a non-singular transformation from a 2D plane to a 2D plane, the dimension of the image is 2. This dimension is equal to the **rank** of the corresponding matrix.

### Singular Transformations

* A linear transformation is **singular** if its image collapses the input space to a lower dimension. It does not cover the entire output space.
* **Geometric Interpretation**: If a transformation maps a basis to vectors that do not span the entire plane (e.g., they all lie on a line or collapse to a single point), it is singular.
* **Case 1: Image is a Line**: Example: Following matrix transforms any input vector such that the output vectors all lie on a single line. For instance:

$$
\begin{bmatrix}
1 & 1 \\
2 & 2
\end{bmatrix}
$$

$$
\begin{bmatrix}
1 & 1 \\
2 & 2
\end{bmatrix} \cdot \begin{bmatrix}
1 \\
0
\end{bmatrix} = \begin{bmatrix}
1 \\
2
\end{bmatrix}
$$

$$
\begin{bmatrix}
1 & 1 \\
2 & 2
\end{bmatrix} \cdot \begin{bmatrix}
0 \\
1
\end{bmatrix} = \begin{bmatrix}
1 \\
2
\end{bmatrix}
$$

$$
\begin{bmatrix}
1 & 1 \\
2 & 2
\end{bmatrix} \cdot \begin{bmatrix}
1 \\
1
\end{bmatrix} = \begin{bmatrix}
2 \\
4
\end{bmatrix}
$$

The original square grid is mapped to a "degenerate parallelogram" (a line segment). The image is a line.  
**Rank**: The dimension of the image (a line) is 1, which is the rank of the matrix.

* **Case 2: Image is a Point**:
  * Example: Following matrix transforms any input vector to the origin (0,0).

$$
\begin{bmatrix}
0 & 0 \\
0 & 0
\end{bmatrix}
$$

$$
\begin{bmatrix}
0 & 0 \\
0 & 0
\end{bmatrix} \cdot \begin{bmatrix}
x \\
y
\end{bmatrix} = \begin{bmatrix}
0 \\
0
\end{bmatrix}
$$

The entire plane collapses to a single point.  
**Rank**: The dimension of the image (a point) is 0, which is the rank of the matrix.

### Rank of a Linear Transformation

* The **rank** of a linear transformation (or its corresponding matrix) is the **dimension of its image**.
* This provides a direct way to identify the rank by observing the dimensionality of the space covered by the transformation's output.

