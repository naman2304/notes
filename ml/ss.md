## Week 4: Linear Transformations and Their Applications

This week focuses on understanding linear algebra concepts through **linear transformations**, leading up to **Eigenvalues and Eigenvectors** and their application in **Principal Component Analysis (PCA)**.

### Principal Component Analysis (PCA)

* **Dimensionality Reduction**: PCA is a technique to reduce the number of dimensions (features/columns) in a dataset while preserving as much useful information as possible.
* **Simplification**: It simplifies complex datasets, making them easier to use and visualize.
* **How it works**: Imagine data points in a 2D space that mostly lie close to a line. PCA finds this line and projects all data points onto it, transforming a 2D dataset into a 1D dataset with minimal information loss. 
* **Application**: Widely used in data science and machine learning.

### Lesson 1: Characterizing Linear Transformations

This lesson focuses on understanding the properties of linear transformations using determinants.

* **Singularity**:
    * A linear transformation can be **singular** or **non-singular**.
    * **Non-singular transformation**: Transforms a space without collapsing it (e.g., plane to plane).
    * **Singular transformation**: Collapses a space to a lower dimension (e.g., plane to a line).
    * This concept is closely related to the singularity of a matrix.
* **Determinant**:
    * **Geometric Interpretation**: The determinant quantifies how much a linear transformation stretches or shrinks space.
    * **Properties**:
        * Useful for analyzing inverse transformations.
        * Helpful when dealing with a cascade of multiple transformations.
        * Properties exist for determinants of matrix products and inverses.

### Lesson 2: Basis, Span, Eigenvectors, and Eigenvalues (Leading to PCA)

This lesson introduces fundamental concepts required to understand PCA.

* **Basis**:
    * A set of vectors that define a space.
    * Any point in a space can be reached by a linear combination of its basis vectors.
    * Example: Two non-collinear vectors form a basis for a 2D plane.
* **Span**:
    * Describes the space that can be generated by linear combinations of a group of vectors.
    * A single vector spans a line.
    * Two non-collinear vectors span a plane.
* **Eigenvectors and Eigenvalues**:
    * **Eigenvectors**: Special directions (vectors) in a space.
    * **Eigenvalues**: Constants associated with eigenvectors.
    * **Property**: Applying a linear transformation to a point along its eigenvector simply scales the point along that same vector; it doesn't change its direction. This means multiplying by a matrix is equivalent to multiplying by a constant (the eigenvalue) for points on the eigenvector.
    * **Significance**: Eigenvectors help characterize linear transformations and are crucial in many machine learning applications, including PCA.

## Singular and Non-Singular Linear Transformations

Linear transformations, like matrices, can be **singular** or **non-singular**. This property relates to the **image** of the transformation and its **rank**.

### Image of a Linear Transformation

* The **image** of a linear transformation is the set of all possible output points when the transformation is applied to every point in the input space.

### Non-Singular Transformations

* A linear transformation is **non-singular** if its image covers the entire output space.
* **Geometric Interpretation**: If a transformation maps a basis (like a square grid) to a new set of basis vectors that still span the entire plane (forming a parallelogram that covers all points), it is non-singular. Example: Following matrix transforms a grid into a stretched grid that still covers the entire plane.

$$
\begin{bmatrix}
3 & 1 \\
1 & 2
\end{bmatrix}
$$
        
        
* **Rank**: For a non-singular transformation from a 2D plane to a 2D plane, the dimension of the image is 2. This dimension is equal to the **rank** of the corresponding matrix.

### Singular Transformations

* A linear transformation is **singular** if its image collapses the input space to a lower dimension. It does not cover the entire output space.
* **Geometric Interpretation**: If a transformation maps a basis to vectors that do not span the entire plane (e.g., they all lie on a line or collapse to a single point), it is singular.
* **Case 1: Image is a Line**: Example: Following matrix transforms any input vector such that the output vectors all lie on a single line. For instance:

$$
\begin{bmatrix}
1 & 1 \\
2 & 2
\end{bmatrix}
$$

$$
\begin{bmatrix}
1 & 1 \\
2 & 2
\end{bmatrix} \cdot \begin{bmatrix}
1 \\
0
\end{bmatrix} = \begin{bmatrix}
1 \\
2
\end{bmatrix}
$$

$$
\begin{bmatrix}
1 & 1 \\
2 & 2
\end{bmatrix} \cdot \begin{bmatrix}
0 \\
1
\end{bmatrix} = \begin{bmatrix}
1 \\
2
\end{bmatrix}
$$

$$
\begin{bmatrix}
1 & 1 \\
2 & 2
\end{bmatrix} \cdot \begin{bmatrix}
1 \\
1
\end{bmatrix} = \begin{bmatrix}
2 \\
4
\end{bmatrix}
$$

The original square grid is mapped to a "degenerate parallelogram" (a line segment). The image is a line.  
**Rank**: The dimension of the image (a line) is 1, which is the rank of the matrix.

* **Case 2: Image is a Point**:
  * Example: Following matrix transforms any input vector to the origin (0,0).

$$
\begin{bmatrix}
0 & 0 \\
0 & 0
\end{bmatrix}
$$

$$
\begin{bmatrix}
0 & 0 \\
0 & 0
\end{bmatrix} \cdot \begin{bmatrix}
x \\
y
\end{bmatrix} = \begin{bmatrix}
0 \\
0
\end{bmatrix}
$$

The entire plane collapses to a single point.  
**Rank**: The dimension of the image (a point) is 0, which is the rank of the matrix.

<img src="/metadata/transformations.png" width="700" />

### Rank of a Linear Transformation

* The **rank** of a linear transformation (or its corresponding matrix) is the **dimension of its image**.
* This provides a direct way to identify the rank by observing the dimensionality of the space covered by the transformation's output.

## Determinants and Matrix Singularity

* The **determinant** of a matrix is a scalar value that provides information about the properties of the matrix, particularly its singularity.
* In the context of **linear transformations**, the determinant can be intuitively understood as the **scaling factor of area or volume** of the transformed fundamental basis.

### Determinant as Area/Volume

* For a 2x2 matrix, the absolute value of its determinant represents the **area of the parallelogram** formed by the transformation of the unit square.
    * **Example**: For the matrix its determinant is $(3 \times 2) - (1 \times 1) = 6 - 1 = 5$. This matrix transforms a unit square (area = 1) into a parallelogram with an area of 5.

$$
\begin{bmatrix}
3 & 1 \\
1 & 2
\end{bmatrix}
$$
        
* For higher dimensions (e.g., 3x3 matrices), the determinant represents the **volume** of the transformed unit cube.

### Singular vs. Non-Singular Matrices

* A matrix is **singular** if its determinant is **zero**.
    * This implies that the linear transformation "squashes" the space into a lower dimension, resulting in an area or volume of zero.
* A matrix is **non-singular** if its determinant is **non-zero**.
    * This means the linear transformation preserves the dimensionality of the space, and the transformed shape has a non-zero area or volume.
    * The determinant being positive or negative **does not affect singularity**; only whether it's zero or non-zero matters.

**Example 1**: For the matrix its determinant is $(1 \times 2) - (2 \times 1) = 0$. This matrix transforms the unit square into a line segment (a "very skinny parallelogram") which has an area of 0.

$$
\begin{bmatrix}
1 & 2 \\
1 & 2
\end{bmatrix}
$$

**Example 2**: For the matrix its determinant is 0. This transforms the unit square into a single point (0,0), which has an area of 0.

$$
\begin{bmatrix}
0 & 0 \\
0 & 0
\end{bmatrix}
$$

### Negative Determinants

<img src="/metadata/neg_det.png" width="700" />

* A **negative determinant** indicates a **change in orientation** (a "flip" or "reflection") of the transformed space.
* The sign of the determinant depends on the order in which the basis vectors are considered.
    * Conventionally, if the transformation results in the basis vectors being in a **clockwise order**, the "area" is considered negative. If in a **counterclockwise order**, it's positive.
    * **Example**: Permuting columns of a matrix changes the sign of the determinant. If the matrix

$$
\begin{bmatrix}
3 & 1 \\
1 & 2
\end{bmatrix}
$$

has a determinant of 5, then the matrix with permuted columns

$$
\begin{bmatrix}
1 & 3 \\
2 & 1
\end{bmatrix}
$$

will have a determinant of $(1 \times 1) - (3 \times 2) = 1 - 6 = -5$. This corresponds to a flipped orientation of the parallelogram.

### Key Takeaway

* The **singularity of a matrix** is solely determined by whether its **determinant is zero or non-zero**.
* A determinant of zero signifies a loss of dimension in the transformation (e.g., a square becoming a line or a point). 
* A non-zero determinant indicates that the transformation preserves the original dimensionality. 

## Determinant of a Product of Matrices

The **determinant of the product of matrices** is equal to the **product of their individual determinants**. This means if you have two matrices, $A$ and $B$, then $det(AB) = det(A) \cdot det(B)$.

For example:

$$
\begin{bmatrix}
3 & 1 \\
1 & 2
\end{bmatrix} \cdot \begin{bmatrix}
1 & 1 \\
-2 & 1
\end{bmatrix} = \begin{bmatrix}
(3 \times 1) + (1 \times -2) & (3 \times 1) + (1 \times 1) \\
(1 \times 1) + (2 \times -2) & (1 \times 1) + (2 \times 1)
\end{bmatrix} = \begin{bmatrix}
1 & 4 \\
-3 & 3
\end{bmatrix}
$$

* $det(A) = (3 \times 2) - (1 \times 1) = 6 - 1 = 5$
* $det(B) = (1 \times 1) - (1 \times -2) = 1 - (-2) = 3$
* $det(AB) = (1 \times 3) - (4 \times -3) = 3 - (-12) = 15$

Notice that $5 \times 3 = 15$.

### Intuition with Linear Transformations

This rule makes intuitive sense when considering linear transformations:

* **Determinant as an Area Scaling Factor:** The determinant of a matrix represents the factor by which the corresponding linear transformation scales areas (or volumes in higher dimensions).
* **Composition of Transformations:** When you multiply two matrices, you are essentially composing two linear transformations.
* **Combined Scaling:** If the first transformation scales areas by a factor of $det(A)$ and the second scales by $det(B)$, then the combined transformation scales areas by $det(A) \cdot det(B)$.

## Singular and Non-Singular Matrices

### Definitions

* **Non-singular Matrix:** A matrix with a non-zero determinant. This means its linear transformation preserves or scales area, but doesn't collapse it to a lower dimension.
* **Singular Matrix:** A matrix with a zero determinant. This means its linear transformation collapses the area to zero (e.g., mapping a 2D plane to a line or point).

### Product of Singular and Non-Singular Matrices

If one of the matrices in a product is singular, then the product of the matrices will **always be singular**.

* **Mathematical Proof:** If $A$ is non-singular and $B$ is singular, then $det(B) = 0$.
    Since $det(AB) = det(A) \cdot det(B)$, substituting $det(B) = 0$ gives $det(AB) = det(A) \cdot 0 = 0$.
    Therefore, $AB$ is singular.

* **Analogy with Numbers:** Just like any number multiplied by zero results in zero, any matrix multiplied by a singular matrix (which has a determinant of zero) results in a singular matrix (which also has a determinant of zero).

* **Geometric Intuition:** A singular matrix "flattens" or "collapses" space. If you perform a transformation that collapses space to a lower dimension (e.g., a 2D plane to a line), and then apply another transformation, the space remains collapsed. Any initial area will ultimately be scaled by zero, resulting in zero area.

## Determinant of an Inverse Matrix

The **determinant of an inverse matrix** is the **reciprocal of the determinant of the original matrix**. This rule applies only if the original matrix is invertible.

Mathematically, for an invertible matrix $A$:

$det(A^{-1}) = \frac{1}{det(A)}$

### Example

* If $det(A) = 5$, then $det(A^{-1}) = \frac{1}{5} = 0.2$.
* If $det(A) = 8$, then $det(A^{-1}) = \frac{1}{8} = 0.125$.

### Proof

The rule can be derived from the **determinant product rule**:

1.  We know that for any two matrices $A$ and $B$, $det(AB) = det(A) \cdot det(B)$.
2.  Let $B$ be the inverse of $A$, i.e., $B = A^{-1}$.
3.  Substitute $B$ with $A^{-1}$ into the product rule:
    $det(A \cdot A^{-1}) = det(A) \cdot det(A^{-1})$
4.  The product of a matrix and its inverse is the **identity matrix** ($I$):
    $A \cdot A^{-1} = I$
5.  So, the equation becomes:
    $det(I) = det(A) \cdot det(A^{-1})$
6.  The **determinant of an identity matrix** is always 1. For a 2x2 identity matrix: $det(I) = (1 \times 1) - (0 \times 0) = 1$.

$$
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}
$$

7.  Substituting $det(I) = 1$ into the equation:
    $1 = det(A) \cdot det(A^{-1})$
8.  Finally, solving for $det(A^{-1})$:
    $det(A^{-1}) = \frac{1}{det(A)}$

### Singular Matrices and Inverses

* A **singular matrix** has a determinant of 0.
* A singular matrix **does not have an inverse**. This is consistent with the determinant rule for inverses, as division by zero is undefined. If $det(A) = 0$, then $\frac{1}{det(A)}$ would be undefined, indicating no inverse exists.
