## Week 4: Linear Transformations and Their Applications

This week focuses on understanding linear algebra concepts through **linear transformations**, leading up to **Eigenvalues and Eigenvectors** and their application in **Principal Component Analysis (PCA)**.

### Principal Component Analysis (PCA)

* **Dimensionality Reduction**: PCA is a technique to reduce the number of dimensions (features/columns) in a dataset while preserving as much useful information as possible.
* **Simplification**: It simplifies complex datasets, making them easier to use and visualize.
* **How it works**: Imagine data points in a 2D space that mostly lie close to a line. PCA finds this line and projects all data points onto it, transforming a 2D dataset into a 1D dataset with minimal information loss. 
* **Application**: Widely used in data science and machine learning.

### Lesson 1: Characterizing Linear Transformations

This lesson focuses on understanding the properties of linear transformations using determinants.

* **Singularity**:
    * A linear transformation can be **singular** or **non-singular**.
    * **Non-singular transformation**: Transforms a space without collapsing it (e.g., plane to plane).
    * **Singular transformation**: Collapses a space to a lower dimension (e.g., plane to a line).
    * This concept is closely related to the singularity of a matrix.
* **Determinant**:
    * **Geometric Interpretation**: The determinant quantifies how much a linear transformation stretches or shrinks space.
    * **Properties**:
        * Useful for analyzing inverse transformations.
        * Helpful when dealing with a cascade of multiple transformations.
        * Properties exist for determinants of matrix products and inverses.

### Lesson 2: Basis, Span, Eigenvectors, and Eigenvalues (Leading to PCA)

This lesson introduces fundamental concepts required to understand PCA.

* **Basis**:
    * A set of vectors that define a space.
    * Any point in a space can be reached by a linear combination of its basis vectors.
    * Example: Two non-collinear vectors form a basis for a 2D plane.
* **Span**:
    * Describes the space that can be generated by linear combinations of a group of vectors.
    * A single vector spans a line.
    * Two non-collinear vectors span a plane.
* **Eigenvectors and Eigenvalues**:
    * **Eigenvectors**: Special directions (vectors) in a space.
    * **Eigenvalues**: Constants associated with eigenvectors.
    * **Property**: Applying a linear transformation to a point along its eigenvector simply scales the point along that same vector; it doesn't change its direction. This means multiplying by a matrix is equivalent to multiplying by a constant (the eigenvalue) for points on the eigenvector.
    * **Significance**: Eigenvectors help characterize linear transformations and are crucial in many machine learning applications, including PCA.

## Singular and Non-Singular Linear Transformations

Linear transformations, like matrices, can be **singular** or **non-singular**. This property relates to the **image** of the transformation and its **rank**.

### Image of a Linear Transformation

* The **image** of a linear transformation is the set of all possible output points when the transformation is applied to every point in the input space.

### Non-Singular Transformations

* A linear transformation is **non-singular** if its image covers the entire output space.
* **Geometric Interpretation**: If a transformation maps a basis (like a square grid) to a new set of basis vectors that still span the entire plane (forming a parallelogram that covers all points), it is non-singular. Example: Following matrix transforms a grid into a stretched grid that still covers the entire plane.

$$
\begin{bmatrix}
3 & 1 \\
1 & 2
\end{bmatrix}
$$
        
        
* **Rank**: For a non-singular transformation from a 2D plane to a 2D plane, the dimension of the image is 2. This dimension is equal to the **rank** of the corresponding matrix.

### Singular Transformations

* A linear transformation is **singular** if its image collapses the input space to a lower dimension. It does not cover the entire output space.
* **Geometric Interpretation**: If a transformation maps a basis to vectors that do not span the entire plane (e.g., they all lie on a line or collapse to a single point), it is singular.
* **Case 1: Image is a Line**: Example: Following matrix transforms any input vector such that the output vectors all lie on a single line. For instance:

$$
\begin{bmatrix}
1 & 1 \\
2 & 2
\end{bmatrix}
$$

$$
\begin{bmatrix}
1 & 1 \\
2 & 2
\end{bmatrix} \cdot \begin{bmatrix}
1 \\
0
\end{bmatrix} = \begin{bmatrix}
1 \\
2
\end{bmatrix}
$$

$$
\begin{bmatrix}
1 & 1 \\
2 & 2
\end{bmatrix} \cdot \begin{bmatrix}
0 \\
1
\end{bmatrix} = \begin{bmatrix}
1 \\
2
\end{bmatrix}
$$

$$
\begin{bmatrix}
1 & 1 \\
2 & 2
\end{bmatrix} \cdot \begin{bmatrix}
1 \\
1
\end{bmatrix} = \begin{bmatrix}
2 \\
4
\end{bmatrix}
$$

The original square grid is mapped to a "degenerate parallelogram" (a line segment). The image is a line.  
**Rank**: The dimension of the image (a line) is 1, which is the rank of the matrix.

* **Case 2: Image is a Point**:
  * Example: Following matrix transforms any input vector to the origin (0,0).

$$
\begin{bmatrix}
0 & 0 \\
0 & 0
\end{bmatrix}
$$

$$
\begin{bmatrix}
0 & 0 \\
0 & 0
\end{bmatrix} \cdot \begin{bmatrix}
x \\
y
\end{bmatrix} = \begin{bmatrix}
0 \\
0
\end{bmatrix}
$$

The entire plane collapses to a single point.  
**Rank**: The dimension of the image (a point) is 0, which is the rank of the matrix.

<img src="/metadata/transformations.png" width="700" />

### Rank of a Linear Transformation

* The **rank** of a linear transformation (or its corresponding matrix) is the **dimension of its image**.
* This provides a direct way to identify the rank by observing the dimensionality of the space covered by the transformation's output.

## Determinants and Matrix Singularity

* The **determinant** of a matrix is a scalar value that provides information about the properties of the matrix, particularly its singularity.
* In the context of **linear transformations**, the determinant can be intuitively understood as the **scaling factor of area or volume** of the transformed fundamental basis.

### Determinant as Area/Volume

* For a 2x2 matrix, the absolute value of its determinant represents the **area of the parallelogram** formed by the transformation of the unit square.
    * **Example**: For the matrix its determinant is $(3 \times 2) - (1 \times 1) = 6 - 1 = 5$. This matrix transforms a unit square (area = 1) into a parallelogram with an area of 5.

$$
\begin{bmatrix}
3 & 1 \\
1 & 2
\end{bmatrix}
$$
        
* For higher dimensions (e.g., 3x3 matrices), the determinant represents the **volume** of the transformed unit cube.

### Singular vs. Non-Singular Matrices

* A matrix is **singular** if its determinant is **zero**.
    * This implies that the linear transformation "squashes" the space into a lower dimension, resulting in an area or volume of zero.
* A matrix is **non-singular** if its determinant is **non-zero**.
    * This means the linear transformation preserves the dimensionality of the space, and the transformed shape has a non-zero area or volume.
    * The determinant being positive or negative **does not affect singularity**; only whether it's zero or non-zero matters.

**Example 1**: For the matrix its determinant is $(1 \times 2) - (2 \times 1) = 0$. This matrix transforms the unit square into a line segment (a "very skinny parallelogram") which has an area of 0.

$$
\begin{bmatrix}
1 & 2 \\
1 & 2
\end{bmatrix}
$$

**Example 2**: For the matrix its determinant is 0. This transforms the unit square into a single point (0,0), which has an area of 0.

$$
\begin{bmatrix}
0 & 0 \\
0 & 0
\end{bmatrix}
$$

### Negative Determinants

<img src="/metadata/neg_det.png" width="700" />

* A **negative determinant** indicates a **change in orientation** (a "flip" or "reflection") of the transformed space.
* The sign of the determinant depends on the order in which the basis vectors are considered.
    * Conventionally, if the transformation results in the basis vectors being in a **clockwise order**, the "area" is considered negative. If in a **counterclockwise order**, it's positive.
    * **Example**: Permuting columns of a matrix changes the sign of the determinant. If the matrix

$$
\begin{bmatrix}
3 & 1 \\
1 & 2
\end{bmatrix}
$$

has a determinant of 5, then the matrix with permuted columns

$$
\begin{bmatrix}
1 & 3 \\
2 & 1
\end{bmatrix}
$$

will have a determinant of $(1 \times 1) - (3 \times 2) = 1 - 6 = -5$. This corresponds to a flipped orientation of the parallelogram.

### Key Takeaway

* The **singularity of a matrix** is solely determined by whether its **determinant is zero or non-zero**.
* A determinant of zero signifies a loss of dimension in the transformation (e.g., a square becoming a line or a point). 
* A non-zero determinant indicates that the transformation preserves the original dimensionality. 

## Determinant of a Product of Matrices

The **determinant of the product of matrices** is equal to the **product of their individual determinants**. This means if you have two matrices, $A$ and $B$, then $det(AB) = det(A) \cdot det(B)$.

For example:

$$
\begin{bmatrix}
3 & 1 \\
1 & 2
\end{bmatrix} \cdot \begin{bmatrix}
1 & 1 \\
-2 & 1
\end{bmatrix} = \begin{bmatrix}
(3 \times 1) + (1 \times -2) & (3 \times 1) + (1 \times 1) \\
(1 \times 1) + (2 \times -2) & (1 \times 1) + (2 \times 1)
\end{bmatrix} = \begin{bmatrix}
1 & 4 \\
-3 & 3
\end{bmatrix}
$$

* $det(A) = (3 \times 2) - (1 \times 1) = 6 - 1 = 5$
* $det(B) = (1 \times 1) - (1 \times -2) = 1 - (-2) = 3$
* $det(AB) = (1 \times 3) - (4 \times -3) = 3 - (-12) = 15$

Notice that $5 \times 3 = 15$.

### Intuition with Linear Transformations

This rule makes intuitive sense when considering linear transformations:

* **Determinant as an Area Scaling Factor:** The determinant of a matrix represents the factor by which the corresponding linear transformation scales areas (or volumes in higher dimensions).
* **Composition of Transformations:** When you multiply two matrices, you are essentially composing two linear transformations.
* **Combined Scaling:** If the first transformation scales areas by a factor of $det(A)$ and the second scales by $det(B)$, then the combined transformation scales areas by $det(A) \cdot det(B)$.

## Singular and Non-Singular Matrices

### Definitions

* **Non-singular Matrix:** A matrix with a non-zero determinant. This means its linear transformation preserves or scales area, but doesn't collapse it to a lower dimension.
* **Singular Matrix:** A matrix with a zero determinant. This means its linear transformation collapses the area to zero (e.g., mapping a 2D plane to a line or point).

### Product of Singular and Non-Singular Matrices

If one of the matrices in a product is singular, then the product of the matrices will **always be singular**.

* **Mathematical Proof:** If $A$ is non-singular and $B$ is singular, then $det(B) = 0$.
    Since $det(AB) = det(A) \cdot det(B)$, substituting $det(B) = 0$ gives $det(AB) = det(A) \cdot 0 = 0$.
    Therefore, $AB$ is singular.

* **Analogy with Numbers:** Just like any number multiplied by zero results in zero, any matrix multiplied by a singular matrix (which has a determinant of zero) results in a singular matrix (which also has a determinant of zero).

* **Geometric Intuition:** A singular matrix "flattens" or "collapses" space. If you perform a transformation that collapses space to a lower dimension (e.g., a 2D plane to a line), and then apply another transformation, the space remains collapsed. Any initial area will ultimately be scaled by zero, resulting in zero area.

## Determinant of an Inverse Matrix

The **determinant of an inverse matrix** is the **reciprocal of the determinant of the original matrix**. This rule applies only if the original matrix is invertible.

Mathematically, for an invertible matrix $A$:

$det(A^{-1}) = \frac{1}{det(A)}$

### Example

* If $det(A) = 5$, then $det(A^{-1}) = \frac{1}{5} = 0.2$.
* If $det(A) = 8$, then $det(A^{-1}) = \frac{1}{8} = 0.125$.

### Proof

The rule can be derived from the **determinant product rule**:

1.  We know that for any two matrices $A$ and $B$, $det(AB) = det(A) \cdot det(B)$.
2.  Let $B$ be the inverse of $A$, i.e., $B = A^{-1}$.
3.  Substitute $B$ with $A^{-1}$ into the product rule:
    $det(A \cdot A^{-1}) = det(A) \cdot det(A^{-1})$
4.  The product of a matrix and its inverse is the **identity matrix** ($I$):
    $A \cdot A^{-1} = I$
5.  So, the equation becomes:
    $det(I) = det(A) \cdot det(A^{-1})$
6.  The **determinant of an identity matrix** is always 1. For a 2x2 identity matrix: $det(I) = (1 \times 1) - (0 \times 0) = 1$.

$$
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}
$$

7.  Substituting $det(I) = 1$ into the equation:
    $1 = det(A) \cdot det(A^{-1})$
8.  Finally, solving for $det(A^{-1})$:
    $det(A^{-1}) = \frac{1}{det(A)}$

### Singular Matrices and Inverses

* A **singular matrix** has a determinant of 0.
* A singular matrix **does not have an inverse**. This is consistent with the determinant rule for inverses, as division by zero is undefined. If $det(A) = 0$, then $\frac{1}{det(A)}$ would be undefined, indicating no inverse exists.

## Basis of a Vector Space

A **basis** in linear algebra is a set of vectors that can describe any point in a given space. It's a fundamental concept for understanding linear transformations and vector spaces.

### Defining a Basis

* A basis is not about the four corners of a square or parallelogram, but rather the **two vectors** originating from the origin that define them.
* The **main property** of a basis is that every point in the space can be expressed as a **linear combination** of its constituent vectors.
    * This means you can reach any point by scaling and summing the basis vectors.
    * Scaling can involve positive or negative values (walking forwards or backwards), and fractional steps are also allowed.

### Examples of Basis

* Any two non-collinear vectors in a 2D plane can form a basis.
* For example, if you have two vectors pointing in different directions, you can reach any point in the plane by combining them.

### Examples of Non-Basis

* Vectors that are **collinear** (lie on the same line) do **not** form a basis for a 2D plane.
    * If two vectors point in the same or opposite directions, they can only cover that specific line, not the entire plane. You can't reach points off that line using only those two directions.

## The Span of Vectors

* The **span of a set of vectors** is the collection of all points reachable by combining those vectors (walking in their directions).
* **Examples of Span:**
    * Two non-collinear vectors in a plane span the **plane**.
    * Two collinear vectors (pointing in the same or opposite directions) span a **line**.
    * A single vector spans the **line** containing it and passing through the origin.

## Basis of a Vector Space

* A **basis** is a **minimal spanning set** for a vector space.
    * **Minimal** means you cannot remove any vector from the set and still span the same space.
    * If a set of vectors spans a space but has "too many" vectors (some are redundant), it's a spanning set but not a basis.
* The **number of vectors in a basis** of a space is equal to the **dimension of that space**.
    * A line (1D space) has a basis of length 1.
    * A plane (2D space) has a basis of length 2.
    * Any basis for a given space will always have the same number of elements.

## Linear Independence and Dependence

* A group of vectors is **linearly independent** if no vector in the group can be expressed as a linear combination of the others.
* A group of vectors is **linearly dependent** if at least one vector in the group can be obtained as a linear combination of the others.
    * If adding a new vector to a set does not change the span, the new set is linearly dependent.
    * If you have more vectors than the dimension of the space you are trying to span, the set will always be linearly dependent.
        * e.g., three or more vectors in a 2D plane are always linearly dependent.
        * e.g., four or more vectors in 3D space are always linearly dependent.

### Checking for Linear Dependence

* To check if vectors $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_k$ are linearly dependent, try to find non-zero constants $\alpha_1, \alpha_2, \dots, \alpha_k$ such that:

$$
\alpha_1\mathbf{v}_1 + \alpha_2\mathbf{v}_2 + \dots + \alpha_k\mathbf{v}_k = \mathbf{0}
$$

If such non-zero constants exist, the vectors are linearly dependent. Equivalently, you can try to express one vector as a linear combination of the others.

* **Example:** Given

$$
\alpha\mathbf{v}_1 + \beta\mathbf{v}_2 = \mathbf{v}_3
$$

$$
\alpha \begin{bmatrix} -1 \\ 1 \end{bmatrix} + \beta \begin{bmatrix} 2 \\ 1 \end{bmatrix} = \begin{bmatrix} -5 \\ 3 \end{bmatrix}
$$

This leads to the system of equations:
* $-\alpha + 2\beta = -5$ (Equation 1)
* $\alpha + \beta = 3$ (Equation 2)

Adding Equation 1 and Equation 2:  
$(- \alpha + 2\beta) + (\alpha + \beta) = -5 + 3$  
$3\beta = -2$  
$\beta = -2/3$  

Substitute $\beta$ into Equation 2:  
$\alpha + (-2/3) = 3$  
$\alpha = 3 + 2/3$  
$\alpha = 11/3$

Since we found a solution ($\alpha = 11/3, \beta = -2/3$), the set $\{\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3\}$ is **linearly dependent**.

$$
\frac{11}{3} \begin{bmatrix} -1 \\ 1 \end{bmatrix} - \frac{2}{3} \begin{bmatrix} 2 \\ 1 \end{bmatrix} = \begin{bmatrix} -11/3 - 4/3 \\ 11/3 - 2/3 \end{bmatrix} = \begin{bmatrix} -15/3 \\ 9/3 \end{bmatrix} = \begin{bmatrix} -5 \\ 3 \end{bmatrix}
$$

## Formal Definition of a Basis

A set of vectors forms a basis for a vector space if it satisfies two conditions:
1.  The set must **span** the vector space.
2.  The vectors in the set must be **linearly independent**.

* It's crucial to remember that not all sets of $n$ vectors will form a basis for an $n$-dimensional space. They must also be linearly independent.

## Eigenbasis: The Special Basis

* The **eigenbasis** is a particularly useful basis, especially in machine learning applications like **Principal Component Analysis (PCA)**.

### How Eigenbasis Works

* Consider a linear transformation represented by a matrix.
* **Standard Basis:** When a transformation acts on the fundamental (standard) basis vectors (e.g., $[1, 0]$ and $[0, 1]$), it typically transforms a square into a parallelogram. This is a **change of coordinates** or **change of basis**.

$$
\begin{bmatrix}
2 & 1 \\
0 & 3
\end{bmatrix} \cdot \begin{bmatrix}
1 \\
0
\end{bmatrix} = \begin{bmatrix}
2 \\
0
\end{bmatrix}
$$

$$
\begin{bmatrix}
2 & 1 \\
0 & 3
\end{bmatrix} \cdot \begin{bmatrix}
0 \\
1
\end{bmatrix} = \begin{bmatrix}
1 \\
3
\end{bmatrix}
$$

* **Eigenbasis:** A special basis can be chosen such that the linear transformation only results in **stretching** along the directions of the basis vectors, without changing their orientation.
    * For the following matrix, if we choose the basis vectors $[1, 0]$ and $[1, 1]$:
        * The vector $[1, 0]$ transforms to $[2, 0]$ (stretched by 2).
        * The vector $[1, 1]$ transforms to $[3, 3]$ (stretched by 3).

$$
\begin{bmatrix}
2 & 1 \\
0 & 3
\end{bmatrix}
$$
 
$$
\begin{bmatrix}
2 & 1 \\
0 & 3
\end{bmatrix} \cdot \begin{bmatrix}
1 \\
0
\end{bmatrix} = \begin{bmatrix}
2 \\
0
\end{bmatrix}
$$

$$
\begin{bmatrix}
2 & 1 \\
0 & 3
\end{bmatrix} \cdot \begin{bmatrix}
1 \\
1
\end{bmatrix} = \begin{bmatrix}
3 \\
3
\end{bmatrix}
$$

* In an eigenbasis, a parallelogram is transformed into another parallelogram with **sides parallel** to the original one. This means the transformation is simply a stretching along these specific directions.

### Eigenvectors and Eigenvalues

* The special vectors in the eigenbasis are called **eigenvectors**.
* The stretching factors corresponding to these eigenvectors are called **eigenvalues**.
    * In the example above, $[1, 0]$ and $[1, 1]$ are eigenvectors, and $2$ and $3$ are their respective eigenvalues.

### Usefulness of Eigenbasis

* Eigenbasis **simplifies linear transformations** by reducing them to simple stretchings along specific directions.
* This significantly simplifies calculations, as finding the image of a point becomes a matter of applying the stretching factors to its components in the eigenbasis, rather than a full matrix multiplication.
    * Example: To find the image of point $[3, 2]$ using the eigenbasis defined by $[1, 0]$ and $[1, 1]$:
        * First, express $[3, 2]$ as a linear combination of the eigenvectors: $1 \cdot [1, 0] + 2 \cdot [1, 1]$.
        * Then, apply the stretching: $1 \cdot (2 \cdot [1, 0]) + 2 \cdot (3 \cdot [1, 1]) = 2 \cdot [1, 0] + 6 \cdot [1, 1] = [2, 0] + [6, 6] = [8, 6]$.
