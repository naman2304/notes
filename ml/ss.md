## Week 4: Linear Transformations and Their Applications

This week focuses on understanding linear algebra concepts through **linear transformations**, leading up to **Eigenvalues and Eigenvectors** and their application in **Principal Component Analysis (PCA)**.

### Principal Component Analysis (PCA)

* **Dimensionality Reduction**: PCA is a technique to reduce the number of dimensions (features/columns) in a dataset while preserving as much useful information as possible.
* **Simplification**: It simplifies complex datasets, making them easier to use and visualize.
* **How it works**: Imagine data points in a 2D space that mostly lie close to a line. PCA finds this line and projects all data points onto it, transforming a 2D dataset into a 1D dataset with minimal information loss. 
* **Application**: Widely used in data science and machine learning.

### Lesson 1: Characterizing Linear Transformations

This lesson focuses on understanding the properties of linear transformations using determinants.

* **Singularity**:
    * A linear transformation can be **singular** or **non-singular**.
    * **Non-singular transformation**: Transforms a space without collapsing it (e.g., plane to plane).
    * **Singular transformation**: Collapses a space to a lower dimension (e.g., plane to a line).
    * This concept is closely related to the singularity of a matrix.
* **Determinant**:
    * **Geometric Interpretation**: The determinant quantifies how much a linear transformation stretches or shrinks space.
    * **Properties**:
        * Useful for analyzing inverse transformations.
        * Helpful when dealing with a cascade of multiple transformations.
        * Properties exist for determinants of matrix products and inverses.

### Lesson 2: Basis, Span, Eigenvectors, and Eigenvalues (Leading to PCA)

This lesson introduces fundamental concepts required to understand PCA.

* **Basis**:
    * A set of vectors that define a space.
    * Any point in a space can be reached by a linear combination of its basis vectors.
    * Example: Two non-collinear vectors form a basis for a 2D plane.
* **Span**:
    * Describes the space that can be generated by linear combinations of a group of vectors.
    * A single vector spans a line.
    * Two non-collinear vectors span a plane.
* **Eigenvectors and Eigenvalues**:
    * **Eigenvectors**: Special directions (vectors) in a space.
    * **Eigenvalues**: Constants associated with eigenvectors.
    * **Property**: Applying a linear transformation to a point along its eigenvector simply scales the point along that same vector; it doesn't change its direction. This means multiplying by a matrix is equivalent to multiplying by a constant (the eigenvalue) for points on the eigenvector.
    * **Significance**: Eigenvectors help characterize linear transformations and are crucial in many machine learning applications, including PCA.

## Singular and Non-Singular Linear Transformations

Linear transformations, like matrices, can be **singular** or **non-singular**. This property relates to the **image** of the transformation and its **rank**.

### Image of a Linear Transformation

* The **image** of a linear transformation is the set of all possible output points when the transformation is applied to every point in the input space.

### Non-Singular Transformations

* A linear transformation is **non-singular** if its image covers the entire output space.
* **Geometric Interpretation**: If a transformation maps a basis (like a square grid) to a new set of basis vectors that still span the entire plane (forming a parallelogram that covers all points), it is non-singular. Example: Following matrix transforms a grid into a stretched grid that still covers the entire plane.

$$
\begin{bmatrix}
3 & 1 \\
1 & 2
\end{bmatrix}
$$
        
        
* **Rank**: For a non-singular transformation from a 2D plane to a 2D plane, the dimension of the image is 2. This dimension is equal to the **rank** of the corresponding matrix.

### Singular Transformations

* A linear transformation is **singular** if its image collapses the input space to a lower dimension. It does not cover the entire output space.
* **Geometric Interpretation**: If a transformation maps a basis to vectors that do not span the entire plane (e.g., they all lie on a line or collapse to a single point), it is singular.
* **Case 1: Image is a Line**: Example: Following matrix transforms any input vector such that the output vectors all lie on a single line. For instance:

$$
\begin{bmatrix}
1 & 1 \\
2 & 2
\end{bmatrix}
$$

$$
\begin{bmatrix}
1 & 1 \\
2 & 2
\end{bmatrix} \cdot \begin{bmatrix}
1 \\
0
\end{bmatrix} = \begin{bmatrix}
1 \\
2
\end{bmatrix}
$$

$$
\begin{bmatrix}
1 & 1 \\
2 & 2
\end{bmatrix} \cdot \begin{bmatrix}
0 \\
1
\end{bmatrix} = \begin{bmatrix}
1 \\
2
\end{bmatrix}
$$

$$
\begin{bmatrix}
1 & 1 \\
2 & 2
\end{bmatrix} \cdot \begin{bmatrix}
1 \\
1
\end{bmatrix} = \begin{bmatrix}
2 \\
4
\end{bmatrix}
$$

The original square grid is mapped to a "degenerate parallelogram" (a line segment). The image is a line.  
**Rank**: The dimension of the image (a line) is 1, which is the rank of the matrix.

* **Case 2: Image is a Point**:
  * Example: Following matrix transforms any input vector to the origin (0,0).

$$
\begin{bmatrix}
0 & 0 \\
0 & 0
\end{bmatrix}
$$

$$
\begin{bmatrix}
0 & 0 \\
0 & 0
\end{bmatrix} \cdot \begin{bmatrix}
x \\
y
\end{bmatrix} = \begin{bmatrix}
0 \\
0
\end{bmatrix}
$$

The entire plane collapses to a single point.  
**Rank**: The dimension of the image (a point) is 0, which is the rank of the matrix.

<img src="/metadata/transformations.png" width="700" />

### Rank of a Linear Transformation

* The **rank** of a linear transformation (or its corresponding matrix) is the **dimension of its image**.
* This provides a direct way to identify the rank by observing the dimensionality of the space covered by the transformation's output.

## Determinants and Matrix Singularity

* The **determinant** of a matrix is a scalar value that provides information about the properties of the matrix, particularly its singularity.
* In the context of **linear transformations**, the determinant can be intuitively understood as the **scaling factor of area or volume** of the transformed fundamental basis.

### Determinant as Area/Volume

* For a 2x2 matrix, the absolute value of its determinant represents the **area of the parallelogram** formed by the transformation of the unit square.
    * **Example**: For the matrix its determinant is $(3 \times 2) - (1 \times 1) = 6 - 1 = 5$. This matrix transforms a unit square (area = 1) into a parallelogram with an area of 5.

$$
\begin{bmatrix}
3 & 1 \\
1 & 2
\end{bmatrix}
$$
        
* For higher dimensions (e.g., 3x3 matrices), the determinant represents the **volume** of the transformed unit cube.

### Singular vs. Non-Singular Matrices

* A matrix is **singular** if its determinant is **zero**.
    * This implies that the linear transformation "squashes" the space into a lower dimension, resulting in an area or volume of zero.
* A matrix is **non-singular** if its determinant is **non-zero**.
    * This means the linear transformation preserves the dimensionality of the space, and the transformed shape has a non-zero area or volume.
    * The determinant being positive or negative **does not affect singularity**; only whether it's zero or non-zero matters.

**Example 1**: For the matrix its determinant is $(1 \times 2) - (2 \times 1) = 0$. This matrix transforms the unit square into a line segment (a "very skinny parallelogram") which has an area of 0.

$$
\begin{bmatrix}
1 & 2 \\
1 & 2
\end{bmatrix}
$$

**Example 2**: For the matrix its determinant is 0. This transforms the unit square into a single point (0,0), which has an area of 0.

$$
\begin{bmatrix}
0 & 0 \\
0 & 0
\end{bmatrix}
$$

### Negative Determinants

<img src="/metadata/neg_det.png" width="700" />

* A **negative determinant** indicates a **change in orientation** (a "flip" or "reflection") of the transformed space.
* The sign of the determinant depends on the order in which the basis vectors are considered.
    * Conventionally, if the transformation results in the basis vectors being in a **clockwise order**, the "area" is considered negative. If in a **counterclockwise order**, it's positive.
    * **Example**: Permuting columns of a matrix changes the sign of the determinant. If the matrix

$$
\begin{bmatrix}
3 & 1 \\
1 & 2
\end{bmatrix}
$$

has a determinant of 5, then the matrix with permuted columns

$$
\begin{bmatrix}
1 & 3 \\
2 & 1
\end{bmatrix}
$$

will have a determinant of $(1 \times 1) - (3 \times 2) = 1 - 6 = -5$. This corresponds to a flipped orientation of the parallelogram.

### Key Takeaway

* The **singularity of a matrix** is solely determined by whether its **determinant is zero or non-zero**.
* A determinant of zero signifies a loss of dimension in the transformation (e.g., a square becoming a line or a point). 
* A non-zero determinant indicates that the transformation preserves the original dimensionality. 

