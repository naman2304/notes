## Week 4: Linear Transformations and Their Applications

This week focuses on understanding linear algebra concepts through **linear transformations**, leading up to **Eigenvalues and Eigenvectors** and their application in **Principal Component Analysis (PCA)**.

### Principal Component Analysis (PCA)

* **Dimensionality Reduction**: PCA is a technique to reduce the number of dimensions (features/columns) in a dataset while preserving as much useful information as possible.
* **Simplification**: It simplifies complex datasets, making them easier to use and visualize.
* **How it works**: Imagine data points in a 2D space that mostly lie close to a line. PCA finds this line and projects all data points onto it, transforming a 2D dataset into a 1D dataset with minimal information loss. 
* **Application**: Widely used in data science and machine learning.

### Lesson 1: Characterizing Linear Transformations

This lesson focuses on understanding the properties of linear transformations using determinants.

* **Singularity**:
    * A linear transformation can be **singular** or **non-singular**.
    * **Non-singular transformation**: Transforms a space without collapsing it (e.g., plane to plane).
    * **Singular transformation**: Collapses a space to a lower dimension (e.g., plane to a line).
    * This concept is closely related to the singularity of a matrix.
* **Determinant**:
    * **Geometric Interpretation**: The determinant quantifies how much a linear transformation stretches or shrinks space.
    * **Properties**:
        * Useful for analyzing inverse transformations.
        * Helpful when dealing with a cascade of multiple transformations.
        * Properties exist for determinants of matrix products and inverses.

### Lesson 2: Basis, Span, Eigenvectors, and Eigenvalues (Leading to PCA)

This lesson introduces fundamental concepts required to understand PCA.

* **Basis**:
    * A set of vectors that define a space.
    * Any point in a space can be reached by a linear combination of its basis vectors.
    * Example: Two non-collinear vectors form a basis for a 2D plane.
* **Span**:
    * Describes the space that can be generated by linear combinations of a group of vectors.
    * A single vector spans a line.
    * Two non-collinear vectors span a plane.
* **Eigenvectors and Eigenvalues**:
    * **Eigenvectors**: Special directions (vectors) in a space.
    * **Eigenvalues**: Constants associated with eigenvectors.
    * **Property**: Applying a linear transformation to a point along its eigenvector simply scales the point along that same vector; it doesn't change its direction. This means multiplying by a matrix is equivalent to multiplying by a constant (the eigenvalue) for points on the eigenvector.
    * **Significance**: Eigenvectors help characterize linear transformations and are crucial in many machine learning applications, including PCA.
